{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a068b3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-19T04:03:09.806024Z",
     "iopub.status.busy": "2025-01-19T04:03:09.805549Z",
     "iopub.status.idle": "2025-01-19T04:03:10.978251Z",
     "shell.execute_reply": "2025-01-19T04:03:10.976643Z"
    },
    "papermill": {
     "duration": 1.187351,
     "end_time": "2025-01-19T04:03:10.981876",
     "exception": false,
     "start_time": "2025-01-19T04:03:09.794525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/2024-hk-1-se-355-p-11-titanic-classification/train.csv\n",
      "/kaggle/input/2024-hk-1-se-355-p-11-titanic-classification/test.csv\n",
      "/kaggle/input/2024-hk-1-se-355-p-11-titanic-classification/gender_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ead9621",
   "metadata": {
    "papermill": {
     "duration": 0.010815,
     "end_time": "2025-01-19T04:03:11.003181",
     "exception": false,
     "start_time": "2025-01-19T04:03:10.992366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Tiền xử lí dữ liệu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee667083",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T04:03:11.026521Z",
     "iopub.status.busy": "2025-01-19T04:03:11.025851Z",
     "iopub.status.idle": "2025-01-19T04:03:13.234323Z",
     "shell.execute_reply": "2025-01-19T04:03:13.232389Z"
    },
    "papermill": {
     "duration": 2.221775,
     "end_time": "2025-01-19T04:03:13.236809",
     "exception": false,
     "start_time": "2025-01-19T04:03:11.015034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data After Processing:\n",
      "   PassengerId  Survived    Pclass       Age     SibSp     Parch      Fare  \\\n",
      "0            1         0  0.827377 -0.565736  0.432793 -0.473674 -0.502445   \n",
      "1            2         1 -1.566107  0.663861  0.432793 -0.473674  0.786845   \n",
      "2            3         1  0.827377 -0.258337 -0.474545 -0.473674 -0.488854   \n",
      "3            4         1 -1.566107  0.433312  0.432793 -0.473674  0.420730   \n",
      "4            5         0  0.827377  0.433312 -0.474545 -0.473674 -0.486337   \n",
      "\n",
      "   Sex_male  Embarked_Q  Embarked_S  ...  Cabin_E8  Cabin_F E69  Cabin_F G63  \\\n",
      "0       1.0         0.0         1.0  ...       0.0          0.0          0.0   \n",
      "1       0.0         0.0         0.0  ...       0.0          0.0          0.0   \n",
      "2       0.0         0.0         1.0  ...       0.0          0.0          0.0   \n",
      "3       0.0         0.0         1.0  ...       0.0          0.0          0.0   \n",
      "4       1.0         0.0         1.0  ...       0.0          0.0          0.0   \n",
      "\n",
      "   Cabin_F G73  Cabin_F2  Cabin_F33  Cabin_F38  Cabin_F4  Cabin_G6  Cabin_T  \n",
      "0          0.0       0.0        0.0        0.0       0.0       0.0      0.0  \n",
      "1          0.0       0.0        0.0        0.0       0.0       0.0      0.0  \n",
      "2          0.0       0.0        0.0        0.0       0.0       0.0      0.0  \n",
      "3          0.0       0.0        0.0        0.0       0.0       0.0      0.0  \n",
      "4          0.0       0.0        0.0        0.0       0.0       0.0      0.0  \n",
      "\n",
      "[5 rows x 156 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Load the train data\n",
    "train_data = pd.read_csv('/kaggle/input/2024-hk-1-se-355-p-11-titanic-classification/train.csv')\n",
    "\n",
    "# Copy data for processing\n",
    "train_processed = train_data.copy()\n",
    "\n",
    "# Step 1: Fill missing values for all columns\n",
    "# Age, Fare: Fill missing values with the median\n",
    "age_imputer = SimpleImputer(strategy='median')\n",
    "fare_imputer = SimpleImputer(strategy='median')\n",
    "train_processed['Age'] = age_imputer.fit_transform(train_processed[['Age']])\n",
    "train_processed['Fare'] = fare_imputer.fit_transform(train_processed[['Fare']])\n",
    "\n",
    "# Embarked, Cabin: Fill missing values with the most frequent value\n",
    "embarked_imputer = SimpleImputer(strategy='most_frequent')\n",
    "cabin_imputer = SimpleImputer(strategy='most_frequent')\n",
    "train_processed['Embarked'] = embarked_imputer.fit_transform(train_processed[['Embarked']]).ravel()\n",
    "train_processed['Cabin'] = cabin_imputer.fit_transform(train_processed[['Cabin']]).ravel()\n",
    "\n",
    "# Step 2: Encode categorical features (Sex, Embarked, Cabin)\n",
    "encoder_sex = OneHotEncoder(drop='first', sparse_output=False)  # Drop first to avoid multicollinearity\n",
    "encoder_embarked = OneHotEncoder(drop='first', sparse_output=False)\n",
    "encoder_cabin = OneHotEncoder(drop='first', sparse_output=False)\n",
    "\n",
    "sex_encoded_train = encoder_sex.fit_transform(train_processed[['Sex']])\n",
    "embarked_encoded_train = encoder_embarked.fit_transform(train_processed[['Embarked']])\n",
    "cabin_encoded_train = encoder_cabin.fit_transform(train_processed[['Cabin']])\n",
    "\n",
    "# Create DataFrames for encoded features\n",
    "sex_encoded_train = pd.DataFrame(sex_encoded_train, columns=encoder_sex.get_feature_names_out(['Sex']))\n",
    "embarked_encoded_train = pd.DataFrame(embarked_encoded_train, columns=encoder_embarked.get_feature_names_out(['Embarked']))\n",
    "cabin_encoded_train = pd.DataFrame(cabin_encoded_train, columns=encoder_cabin.get_feature_names_out(['Cabin']))\n",
    "\n",
    "# Reset index for concatenation\n",
    "sex_encoded_train.reset_index(drop=True, inplace=True)\n",
    "embarked_encoded_train.reset_index(drop=True, inplace=True)\n",
    "cabin_encoded_train.reset_index(drop=True, inplace=True)\n",
    "train_processed.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Step 3: Combine features for training\n",
    "train_final = pd.concat(\n",
    "    [train_processed[['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']], \n",
    "     sex_encoded_train, embarked_encoded_train, cabin_encoded_train],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 4: Standardize the numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_features_train = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "train_final[numerical_features_train] = scaler.fit_transform(train_final[numerical_features_train])\n",
    "\n",
    "# Check the result\n",
    "print(\"Train Data After Processing:\")\n",
    "print(train_final.head())\n",
    "\n",
    "# Save the processed data to a CSV file\n",
    "train_final.to_csv('train_final.csv', index=False)  # Lưu DataFrame vào tệp CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19520734",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T04:03:13.254002Z",
     "iopub.status.busy": "2025-01-19T04:03:13.253378Z",
     "iopub.status.idle": "2025-01-19T04:03:13.354962Z",
     "shell.execute_reply": "2025-01-19T04:03:13.353171Z"
    },
    "papermill": {
     "duration": 0.112413,
     "end_time": "2025-01-19T04:03:13.357231",
     "exception": false,
     "start_time": "2025-01-19T04:03:13.244818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data After Processing:\n",
      "   PassengerId    Pclass       Age     SibSp     Parch      Fare  Sex_male  \\\n",
      "0          892  0.873482  0.386231 -0.499470 -0.400248 -0.497413       1.0   \n",
      "1          893  0.873482  1.371370  0.616992 -0.400248 -0.512278       0.0   \n",
      "2          894 -0.315819  2.553537 -0.499470 -0.400248 -0.464100       1.0   \n",
      "3          895  0.873482 -0.204852 -0.499470 -0.400248 -0.482475       1.0   \n",
      "4          896  0.873482 -0.598908  0.616992  0.619896 -0.417492       0.0   \n",
      "\n",
      "   Embarked_Q  Embarked_S  Cabin_A18  ...  Cabin_E52  Cabin_E60  Cabin_F  \\\n",
      "0         1.0         0.0        0.0  ...        0.0        0.0      0.0   \n",
      "1         0.0         1.0        0.0  ...        0.0        0.0      0.0   \n",
      "2         1.0         0.0        0.0  ...        0.0        0.0      0.0   \n",
      "3         0.0         1.0        0.0  ...        0.0        0.0      0.0   \n",
      "4         0.0         1.0        0.0  ...        0.0        0.0      0.0   \n",
      "\n",
      "   Cabin_F E46  Cabin_F E57  Cabin_F G63  Cabin_F2  Cabin_F33  Cabin_F4  \\\n",
      "0          0.0          0.0          0.0       0.0        0.0       0.0   \n",
      "1          0.0          0.0          0.0       0.0        0.0       0.0   \n",
      "2          0.0          0.0          0.0       0.0        0.0       0.0   \n",
      "3          0.0          0.0          0.0       0.0        0.0       0.0   \n",
      "4          0.0          0.0          0.0       0.0        0.0       0.0   \n",
      "\n",
      "   Cabin_G6  \n",
      "0       0.0  \n",
      "1       0.0  \n",
      "2       0.0  \n",
      "3       0.0  \n",
      "4       0.0  \n",
      "\n",
      "[5 rows x 84 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv('/kaggle/input/2024-hk-1-se-355-p-11-titanic-classification/test.csv')\n",
    "\n",
    "# Copy data for processing\n",
    "test_processed = test_data.copy()\n",
    "\n",
    "# Step 1: Fill missing values for all columns\n",
    "# Age, Fare: Fill missing values with the median\n",
    "age_imputer = SimpleImputer(strategy='median')\n",
    "fare_imputer = SimpleImputer(strategy='median')\n",
    "test_processed['Age'] = age_imputer.fit_transform(test_processed[['Age']])\n",
    "test_processed['Fare'] = fare_imputer.fit_transform(test_processed[['Fare']])\n",
    "\n",
    "# Embarked, Cabin: Fill missing values with the most frequent value\n",
    "embarked_imputer = SimpleImputer(strategy='most_frequent')\n",
    "cabin_imputer = SimpleImputer(strategy='most_frequent')\n",
    "test_processed['Embarked'] = embarked_imputer.fit_transform(test_processed[['Embarked']]).ravel()\n",
    "test_processed['Cabin'] = cabin_imputer.fit_transform(test_processed[['Cabin']]).ravel()\n",
    "\n",
    "# Step 2: Encode categorical features (Sex, Embarked, Cabin)\n",
    "encoder_sex = OneHotEncoder(drop='first', sparse_output=False)  # Drop first to avoid multicollinearity\n",
    "encoder_embarked = OneHotEncoder(drop='first', sparse_output=False)\n",
    "encoder_cabin = OneHotEncoder(drop='first', sparse_output=False)\n",
    "\n",
    "sex_encoded_test = encoder_sex.fit_transform(test_processed[['Sex']])\n",
    "embarked_encoded_test = encoder_embarked.fit_transform(test_processed[['Embarked']])\n",
    "cabin_encoded_test = encoder_cabin.fit_transform(test_processed[['Cabin']])\n",
    "\n",
    "# Create DataFrames for encoded features\n",
    "sex_encoded_test = pd.DataFrame(sex_encoded_test, columns=encoder_sex.get_feature_names_out(['Sex']))\n",
    "embarked_encoded_test = pd.DataFrame(embarked_encoded_test, columns=encoder_embarked.get_feature_names_out(['Embarked']))\n",
    "cabin_encoded_test = pd.DataFrame(cabin_encoded_test, columns=encoder_cabin.get_feature_names_out(['Cabin']))\n",
    "\n",
    "# Reset index for concatenation\n",
    "sex_encoded_test.reset_index(drop=True, inplace=True)\n",
    "embarked_encoded_test.reset_index(drop=True, inplace=True)\n",
    "cabin_encoded_test.reset_index(drop=True, inplace=True)\n",
    "test_processed.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Step 3: Combine features for testing\n",
    "test_final = pd.concat(\n",
    "    [test_processed[['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']], \n",
    "     sex_encoded_test, embarked_encoded_test, cabin_encoded_test],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 4: Standardize the numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_features_test = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "test_final[numerical_features_test] = scaler.fit_transform(test_final[numerical_features_test])\n",
    "\n",
    "# Check the result\n",
    "print(\"Test Data After Processing:\")\n",
    "print(test_final.head())\n",
    "\n",
    "# Save the processed data to a CSV file\n",
    "test_final.to_csv('test_final.csv', index=False)  # Lưu DataFrame vào tệp CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc61c4c",
   "metadata": {
    "papermill": {
     "duration": 0.007025,
     "end_time": "2025-01-19T04:03:13.371875",
     "exception": false,
     "start_time": "2025-01-19T04:03:13.364850",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423324ad",
   "metadata": {
    "papermill": {
     "duration": 0.008659,
     "end_time": "2025-01-19T04:03:13.389194",
     "exception": false,
     "start_time": "2025-01-19T04:03:13.380535",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**1. Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a434c626",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T04:03:13.406010Z",
     "iopub.status.busy": "2025-01-19T04:03:13.405569Z",
     "iopub.status.idle": "2025-01-19T04:03:13.500389Z",
     "shell.execute_reply": "2025-01-19T04:03:13.499091Z"
    },
    "papermill": {
     "duration": 0.105491,
     "end_time": "2025-01-19T04:03:13.502364",
     "exception": false,
     "start_time": "2025-01-19T04:03:13.396873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85       110\n",
      "           1       0.79      0.70      0.74        69\n",
      "\n",
      "    accuracy                           0.81       179\n",
      "   macro avg       0.80      0.79      0.79       179\n",
      "weighted avg       0.81      0.81      0.81       179\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8100558659217877"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the preprocessed train data\n",
    "train_final = pd.read_csv('/kaggle/working/train_final.csv')\n",
    "\n",
    "# Separate features (X) and target (y) from the training dataset\n",
    "X = train_final.drop(columns=['PassengerId', 'Survived'])\n",
    "y = train_final['Survived']\n",
    "\n",
    "# Split into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize and train Logistic Regression model\n",
    "logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation data to evaluate performance\n",
    "y_pred_val = logreg.predict(X_val)\n",
    "\n",
    "# Calculate accuracy on validation data\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "# Print classification report for Logistic Regression model\n",
    "print(f\"Classification Report for Logistic Regression:\")\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "accuracy_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe656bdf",
   "metadata": {
    "papermill": {
     "duration": 0.007951,
     "end_time": "2025-01-19T04:03:13.518326",
     "exception": false,
     "start_time": "2025-01-19T04:03:13.510375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**2. K-Nearest Neighbors (KNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c8514bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T04:03:13.535820Z",
     "iopub.status.busy": "2025-01-19T04:03:13.535332Z",
     "iopub.status.idle": "2025-01-19T04:03:13.664511Z",
     "shell.execute_reply": "2025-01-19T04:03:13.663005Z"
    },
    "papermill": {
     "duration": 0.141823,
     "end_time": "2025-01-19T04:03:13.668119",
     "exception": false,
     "start_time": "2025-01-19T04:03:13.526296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for K-Nearest Neighbors:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.88      0.83       110\n",
      "           1       0.77      0.62      0.69        69\n",
      "\n",
      "    accuracy                           0.78       179\n",
      "   macro avg       0.78      0.75      0.76       179\n",
      "weighted avg       0.78      0.78      0.78       179\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7821229050279329"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the preprocessed train data\n",
    "train_final = pd.read_csv('/kaggle/working/train_final.csv')\n",
    "\n",
    "# Separate features (X) and target (y) from the training dataset\n",
    "X = train_final.drop(columns=['PassengerId', 'Survived'])\n",
    "y = train_final['Survived']\n",
    "\n",
    "# Split into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize and train K-Nearest Neighbors model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation data to evaluate performance\n",
    "y_pred_val = knn_model.predict(X_val)\n",
    "\n",
    "# Calculate accuracy on validation data\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "# Print classification report for K-Nearest Neighbors model\n",
    "print(f\"Classification Report for K-Nearest Neighbors:\")\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "accuracy_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f4bb29",
   "metadata": {
    "papermill": {
     "duration": 0.009381,
     "end_time": "2025-01-19T04:03:13.687252",
     "exception": false,
     "start_time": "2025-01-19T04:03:13.677871",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**3. Decision Tree Classifier (CART)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2496630",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T04:03:13.705675Z",
     "iopub.status.busy": "2025-01-19T04:03:13.705119Z",
     "iopub.status.idle": "2025-01-19T04:03:13.816831Z",
     "shell.execute_reply": "2025-01-19T04:03:13.815206Z"
    },
    "papermill": {
     "duration": 0.123253,
     "end_time": "2025-01-19T04:03:13.819140",
     "exception": false,
     "start_time": "2025-01-19T04:03:13.695887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Decision Tree Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.83       110\n",
      "           1       0.74      0.71      0.73        69\n",
      "\n",
      "    accuracy                           0.79       179\n",
      "   macro avg       0.78      0.78      0.78       179\n",
      "weighted avg       0.79      0.79      0.79       179\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7932960893854749"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the preprocessed train data\n",
    "train_final = pd.read_csv('/kaggle/working/train_final.csv')\n",
    "\n",
    "# Separate features (X) and target (y) from the training dataset\n",
    "X = train_final.drop(columns=['PassengerId', 'Survived'])\n",
    "y = train_final['Survived']\n",
    "\n",
    "# Split into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize and train the Decision Tree model\n",
    "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation data to evaluate performance\n",
    "y_pred_val = decision_tree_model.predict(X_val)\n",
    "\n",
    "# Calculate accuracy on validation data\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "# Print classification report for Decision Tree model\n",
    "print(f\"Classification Report for Decision Tree Classifier:\")\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "accuracy_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9b0a4",
   "metadata": {
    "papermill": {
     "duration": 0.007328,
     "end_time": "2025-01-19T04:03:13.834432",
     "exception": false,
     "start_time": "2025-01-19T04:03:13.827104",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**4. Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bca626ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T04:03:13.851530Z",
     "iopub.status.busy": "2025-01-19T04:03:13.851031Z",
     "iopub.status.idle": "2025-01-19T04:03:14.309175Z",
     "shell.execute_reply": "2025-01-19T04:03:14.307771Z"
    },
    "papermill": {
     "duration": 0.468985,
     "end_time": "2025-01-19T04:03:14.311116",
     "exception": false,
     "start_time": "2025-01-19T04:03:13.842131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.84       110\n",
      "           1       0.75      0.70      0.72        69\n",
      "\n",
      "    accuracy                           0.79       179\n",
      "   macro avg       0.78      0.78      0.78       179\n",
      "weighted avg       0.79      0.79      0.79       179\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7932960893854749"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the preprocessed train data\n",
    "train_final = pd.read_csv('/kaggle/working/train_final.csv')\n",
    "\n",
    "# Separate features (X) and target (y) from the training dataset\n",
    "X = train_final.drop(columns=['PassengerId', 'Survived'])\n",
    "y = train_final['Survived']\n",
    "\n",
    "# Split into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "random_forest_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation data to evaluate performance\n",
    "y_pred_val = random_forest_model.predict(X_val)\n",
    "\n",
    "# Calculate accuracy on validation data\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "# Print classification report for Random Forest model\n",
    "print(f\"Classification Report for Random Forest:\")\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "accuracy_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6508b59",
   "metadata": {
    "papermill": {
     "duration": 0.007499,
     "end_time": "2025-01-19T04:03:14.326830",
     "exception": false,
     "start_time": "2025-01-19T04:03:14.319331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**5. Gradient Boosting (XGBoost)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe5f38ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T04:03:14.344184Z",
     "iopub.status.busy": "2025-01-19T04:03:14.343709Z",
     "iopub.status.idle": "2025-01-19T04:03:14.624556Z",
     "shell.execute_reply": "2025-01-19T04:03:14.623293Z"
    },
    "papermill": {
     "duration": 0.291886,
     "end_time": "2025-01-19T04:03:14.626547",
     "exception": false,
     "start_time": "2025-01-19T04:03:14.334661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.92      0.86       110\n",
      "           1       0.83      0.64      0.72        69\n",
      "\n",
      "    accuracy                           0.81       179\n",
      "   macro avg       0.82      0.78      0.79       179\n",
      "weighted avg       0.81      0.81      0.80       179\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8100558659217877"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the preprocessed train data\n",
    "train_final = pd.read_csv('/kaggle/working/train_final.csv')\n",
    "\n",
    "# Separate features (X) and target (y) from the training dataset\n",
    "X = train_final.drop(columns=['PassengerId', 'Survived'])\n",
    "y = train_final['Survived']\n",
    "\n",
    "# Split into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize the Gradient Boosting model\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation data to evaluate performance\n",
    "y_pred_val = model.predict(X_val)\n",
    "\n",
    "# Calculate accuracy on validation data\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "# Print classification report for the model\n",
    "print(f\"Classification Report for Gradient Boosting:\")\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "accuracy_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d4e2d2",
   "metadata": {
    "papermill": {
     "duration": 0.007466,
     "end_time": "2025-01-19T04:03:14.642663",
     "exception": false,
     "start_time": "2025-01-19T04:03:14.635197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7b2b434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T04:03:14.660378Z",
     "iopub.status.busy": "2025-01-19T04:03:14.659899Z",
     "iopub.status.idle": "2025-01-19T04:03:14.667186Z",
     "shell.execute_reply": "2025-01-19T04:03:14.665745Z"
    },
    "papermill": {
     "duration": 0.018565,
     "end_time": "2025-01-19T04:03:14.669196",
     "exception": false,
     "start_time": "2025-01-19T04:03:14.650631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n",
      "       'Sex_male', 'Embarked_Q', 'Embarked_S',\n",
      "       ...\n",
      "       'Cabin_E8', 'Cabin_F E69', 'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2',\n",
      "       'Cabin_F33', 'Cabin_F38', 'Cabin_F4', 'Cabin_G6', 'Cabin_T'],\n",
      "      dtype='object', length=156)\n",
      "Index(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n",
      "       'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_final.columns)\n",
    "print(test_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13371a29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T04:03:14.688173Z",
     "iopub.status.busy": "2025-01-19T04:03:14.687724Z",
     "iopub.status.idle": "2025-01-19T04:03:15.248651Z",
     "shell.execute_reply": "2025-01-19T04:03:15.247222Z"
    },
    "papermill": {
     "duration": 0.572696,
     "end_time": "2025-01-19T04:03:15.250714",
     "exception": false,
     "start_time": "2025-01-19T04:03:14.678018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns: Index(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',\n",
      "       'Sex_male', 'Embarked_Q', 'Embarked_S',\n",
      "       ...\n",
      "       'Cabin_E8', 'Cabin_F E69', 'Cabin_F G63', 'Cabin_F G73', 'Cabin_F2',\n",
      "       'Cabin_F33', 'Cabin_F38', 'Cabin_F4', 'Cabin_G6', 'Cabin_T'],\n",
      "      dtype='object', length=156)\n",
      "Test columns: Index(['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_male',\n",
      "       'Embarked_Q', 'Embarked_S', 'Cabin_A18', 'Cabin_A21', 'Cabin_A29',\n",
      "       'Cabin_A34', 'Cabin_A9', 'Cabin_B10', 'Cabin_B11', 'Cabin_B24',\n",
      "       'Cabin_B26', 'Cabin_B36', 'Cabin_B41', 'Cabin_B45', 'Cabin_B51 B53 B55',\n",
      "       'Cabin_B52 B54 B56', 'Cabin_B57 B59 B63 B66', 'Cabin_B58 B60',\n",
      "       'Cabin_B61', 'Cabin_B69', 'Cabin_B71', 'Cabin_B78', 'Cabin_C101',\n",
      "       'Cabin_C105', 'Cabin_C106', 'Cabin_C116', 'Cabin_C130', 'Cabin_C132',\n",
      "       'Cabin_C22 C26', 'Cabin_C23 C25 C27', 'Cabin_C28', 'Cabin_C31',\n",
      "       'Cabin_C32', 'Cabin_C39', 'Cabin_C46', 'Cabin_C51', 'Cabin_C53',\n",
      "       'Cabin_C54', 'Cabin_C55 C57', 'Cabin_C6', 'Cabin_C62 C64', 'Cabin_C7',\n",
      "       'Cabin_C78', 'Cabin_C80', 'Cabin_C85', 'Cabin_C86', 'Cabin_C89',\n",
      "       'Cabin_C97', 'Cabin_D', 'Cabin_D10 D12', 'Cabin_D15', 'Cabin_D19',\n",
      "       'Cabin_D21', 'Cabin_D22', 'Cabin_D28', 'Cabin_D30', 'Cabin_D34',\n",
      "       'Cabin_D37', 'Cabin_D38', 'Cabin_D40', 'Cabin_D43', 'Cabin_E31',\n",
      "       'Cabin_E34', 'Cabin_E39 E41', 'Cabin_E45', 'Cabin_E46', 'Cabin_E50',\n",
      "       'Cabin_E52', 'Cabin_E60', 'Cabin_F', 'Cabin_F E46', 'Cabin_F E57',\n",
      "       'Cabin_F G63', 'Cabin_F2', 'Cabin_F33', 'Cabin_F4', 'Cabin_G6'],\n",
      "      dtype='object')\n",
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.89      0.85       110\n",
      "           1       0.79      0.67      0.72        69\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.80      0.78      0.79       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\n",
      "Classification Report for K-Nearest Neighbors:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.90      0.85       110\n",
      "           1       0.81      0.67      0.73        69\n",
      "\n",
      "    accuracy                           0.81       179\n",
      "   macro avg       0.81      0.78      0.79       179\n",
      "weighted avg       0.81      0.81      0.81       179\n",
      "\n",
      "Classification Report for Decision Tree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86       110\n",
      "           1       0.79      0.72      0.76        69\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.81      0.80      0.81       179\n",
      "weighted avg       0.82      0.82      0.82       179\n",
      "\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85       110\n",
      "           1       0.78      0.72      0.75        69\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.81      0.80      0.80       179\n",
      "weighted avg       0.81      0.82      0.81       179\n",
      "\n",
      "Classification Report for Gradient Boosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.89      0.84       110\n",
      "           1       0.79      0.65      0.71        69\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.80      0.77      0.78       179\n",
      "weighted avg       0.80      0.80      0.79       179\n",
      "\n",
      "Submission file saved as 'submission.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the preprocessed train data\n",
    "train_final = pd.read_csv('/kaggle/working/train_final.csv')\n",
    "\n",
    "# Load the preprocessed test data\n",
    "test_data = pd.read_csv('/kaggle/working/test_final.csv')  # Thay thế bằng đường dẫn thực tế đến dữ liệu kiểm tra\n",
    "\n",
    "# Kiểm tra các cột trong train_final và test_data\n",
    "print(\"Train columns:\", train_final.columns)\n",
    "print(\"Test columns:\", test_data.columns)\n",
    "\n",
    "# Separate features (X) and target (y) from the training dataset\n",
    "X = train_final.drop(columns=['PassengerId', 'Survived'])\n",
    "y = train_final['Survived']\n",
    "\n",
    "# Xác định các cột số và cột phân loại\n",
    "numeric_features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "categorical_features = ['Sex_male', 'Embarked_Q', 'Embarked_S']  # Sử dụng các cột đã mã hóa\n",
    "\n",
    "# Tạo một Pipeline để xử lý dữ liệu\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numeric_features),  # Giữ nguyên các cột số\n",
    "        ('cat', 'passthrough', categorical_features)  # Giữ nguyên các cột đã mã hóa\n",
    "    ])\n",
    "\n",
    "# Tạo mô hình với Pipeline\n",
    "models = {\n",
    "    'Logistic Regression': Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                           ('classifier', LogisticRegression(random_state=42, max_iter=1000))]),\n",
    "    'K-Nearest Neighbors': Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                           ('classifier', KNeighborsClassifier(n_neighbors=5))]),\n",
    "    'Decision Tree': Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', DecisionTreeClassifier(random_state=42))]),\n",
    "    'Random Forest': Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('classifier', RandomForestClassifier(random_state=42, n_estimators=100))]),\n",
    "    'Gradient Boosting': Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                          ('classifier', GradientBoostingClassifier(random_state=42))])\n",
    "}\n",
    "\n",
    "# Train and evaluate each model on the validation data\n",
    "results = {}\n",
    "predictions = {}\n",
    "\n",
    "# Split into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    # Predict on validation data to evaluate performance\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    # Calculate accuracy on validation data\n",
    "    accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "    results[name] = accuracy_val\n",
    "\n",
    "    # Store predictions for the test data\n",
    "    y_pred_test = model.predict(test_data.drop(columns=['PassengerId']))  # Dự đoán trên dữ liệu kiểm tra\n",
    "    predictions[name] = y_pred_test\n",
    "\n",
    "    # Print classification report for each model\n",
    "    print(f\"Classification Report for {name}:\")\n",
    "    print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "# Tạo DataFrame cho file nộp kết quả\n",
    "# Giả sử bạn muốn lưu kết quả dự đoán từ mô hình Logistic Regression\n",
    "y_pred = predictions['Gradient Boosting']\n",
    "\n",
    "# Tạo DataFrame cho file nộp kết quả\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_data['PassengerId'],\n",
    "    'Survived': y_pred\n",
    "})\n",
    "\n",
    "# Lưu file nộp kết quả\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Tải file về máy tính (nếu bạn đang sử dụng Jupyter Notebook hoặc Google Colab)\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'submission.csv')\n",
    "\n",
    "print(\"Submission file saved as 'submission.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bce116d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T04:03:15.269554Z",
     "iopub.status.busy": "2025-01-19T04:03:15.269022Z",
     "iopub.status.idle": "2025-01-19T04:03:15.827668Z",
     "shell.execute_reply": "2025-01-19T04:03:15.826056Z"
    },
    "papermill": {
     "duration": 0.570896,
     "end_time": "2025-01-19T04:03:15.830207",
     "exception": false,
     "start_time": "2025-01-19T04:03:15.259311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for XGBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84       110\n",
      "           1       0.75      0.72      0.74        69\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.79      0.79      0.79       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\n",
      "Validation Accuracy: 0.7988826815642458\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the preprocessed train data\n",
    "train_final = pd.read_csv('/kaggle/working/train_final.csv')\n",
    "\n",
    "# Separate features (X) and target (y) from the training dataset\n",
    "X = train_final.drop(columns=['PassengerId', 'Survived'])\n",
    "y = train_final['Survived']\n",
    "\n",
    "# Split into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation data to evaluate performance\n",
    "y_pred_val = model.predict(X_val)\n",
    "\n",
    "# Calculate accuracy on validation data\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "# Print classification report for the model\n",
    "print(f\"Classification Report for XGBoost:\")\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36c93252",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T04:03:15.849389Z",
     "iopub.status.busy": "2025-01-19T04:03:15.848922Z",
     "iopub.status.idle": "2025-01-19T04:03:19.581543Z",
     "shell.execute_reply": "2025-01-19T04:03:19.580131Z"
    },
    "papermill": {
     "duration": 3.745718,
     "end_time": "2025-01-19T04:03:19.584534",
     "exception": false,
     "start_time": "2025-01-19T04:03:15.838816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 273, number of negative: 439\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 211\n",
      "[LightGBM] [Info] Number of data points in the train set: 712, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383427 -> initscore=-0.475028\n",
      "[LightGBM] [Info] Start training from score -0.475028\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Classification Report for LightGBM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83       110\n",
      "           1       0.73      0.68      0.71        69\n",
      "\n",
      "    accuracy                           0.78       179\n",
      "   macro avg       0.77      0.76      0.77       179\n",
      "weighted avg       0.78      0.78      0.78       179\n",
      "\n",
      "Validation Accuracy: 0.7821229050279329\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the preprocessed train data\n",
    "train_final = pd.read_csv('/kaggle/working/train_final.csv')\n",
    "\n",
    "# Separate features (X) and target (y) from the training dataset\n",
    "X = train_final.drop(columns=['PassengerId', 'Survived'])\n",
    "y = train_final['Survived']\n",
    "\n",
    "# Split into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize the LightGBM model\n",
    "model = LGBMClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation data to evaluate performance\n",
    "y_pred_val = model.predict(X_val)\n",
    "\n",
    "# Calculate accuracy on validation data\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "# Print classification report for the model\n",
    "print(f\"Classification Report for LightGBM:\")\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "097bc779",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T04:03:19.604021Z",
     "iopub.status.busy": "2025-01-19T04:03:19.603556Z",
     "iopub.status.idle": "2025-01-19T04:03:46.194381Z",
     "shell.execute_reply": "2025-01-19T04:03:46.193178Z"
    },
    "papermill": {
     "duration": 26.603016,
     "end_time": "2025-01-19T04:03:46.196363",
     "exception": false,
     "start_time": "2025-01-19T04:03:19.593347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "Classification Report for Best XGBoost Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84       110\n",
      "           1       0.77      0.64      0.70        69\n",
      "\n",
      "    accuracy                           0.79       179\n",
      "   macro avg       0.78      0.76      0.77       179\n",
      "weighted avg       0.79      0.79      0.78       179\n",
      "\n",
      "Validation Accuracy: 0.7877094972067039\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Load the preprocessed train data\n",
    "train_final = pd.read_csv('/kaggle/working/train_final.csv')\n",
    "\n",
    "# Separate features (X) and target (y) from the training dataset\n",
    "X = train_final.drop(columns=['PassengerId', 'Survived'])\n",
    "y = train_final['Survived']\n",
    "\n",
    "# Split into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1)\n",
    "\n",
    "# Train the model with GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predict on validation data to evaluate performance\n",
    "y_pred_val = best_model.predict(X_val)\n",
    "\n",
    "# Calculate accuracy on validation data\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "# Print classification report for the model\n",
    "print(f\"Classification Report for Best XGBoost Model:\")\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0e0d6c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T04:03:46.215785Z",
     "iopub.status.busy": "2025-01-19T04:03:46.215317Z",
     "iopub.status.idle": "2025-01-19T04:03:46.348598Z",
     "shell.execute_reply": "2025-01-19T04:03:46.347015Z"
    },
    "papermill": {
     "duration": 0.145335,
     "end_time": "2025-01-19T04:03:46.350723",
     "exception": false,
     "start_time": "2025-01-19T04:03:46.205388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.81      0.81       110\n",
      "           1       0.70      0.71      0.71        69\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.76      0.76      0.76       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "Validation Accuracy: 0.770949720670391\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the preprocessed train data\n",
    "train_final = pd.read_csv('/kaggle/working/train_final.csv')\n",
    "\n",
    "# Separate features (X) and target (y) from the training dataset\n",
    "X = train_final.drop(columns=['PassengerId', 'Survived'])\n",
    "y = train_final['Survived']\n",
    "\n",
    "# Split into training and validation sets (80% train, 20% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Initialize the SVM model\n",
    "model = SVC(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation data to evaluate performance\n",
    "y_pred_val = model.predict(X_val)\n",
    "\n",
    "# Calculate accuracy on validation data\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "# Print classification report for the model\n",
    "print(f\"Classification Report for SVM:\")\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8351bb9d",
   "metadata": {
    "papermill": {
     "duration": 0.008383,
     "end_time": "2025-01-19T04:03:46.368122",
     "exception": false,
     "start_time": "2025-01-19T04:03:46.359739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Test không có cột cain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73815952",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T04:03:46.387604Z",
     "iopub.status.busy": "2025-01-19T04:03:46.387120Z",
     "iopub.status.idle": "2025-01-19T04:03:46.422528Z",
     "shell.execute_reply": "2025-01-19T04:03:46.421235Z"
    },
    "papermill": {
     "duration": 0.047667,
     "end_time": "2025-01-19T04:03:46.424556",
     "exception": false,
     "start_time": "2025-01-19T04:03:46.376889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass   Age  SibSp  Parch     Fare  Sex_male  \\\n",
      "0            1         0       3  22.0      1      0   7.2500      True   \n",
      "1            2         1       1  38.0      1      0  71.2833     False   \n",
      "2            3         1       3  26.0      0      0   7.9250     False   \n",
      "3            4         1       1  35.0      1      0  53.1000     False   \n",
      "4            5         0       3  35.0      0      0   8.0500      True   \n",
      "\n",
      "   Embarked_Q  Embarked_S  \n",
      "0       False        True  \n",
      "1       False       False  \n",
      "2       False        True  \n",
      "3       False        True  \n",
      "4       False        True  \n",
      "[[ 0.82737724 -0.56573646  0.43279337 -0.47367361 -0.50244517  0.73769513\n",
      "  -0.30756234  0.61583843]\n",
      " [-1.56610693  0.66386103  0.43279337 -0.47367361  0.78684529 -1.35557354\n",
      "  -0.30756234 -1.62380254]\n",
      " [ 0.82737724 -0.25833709 -0.4745452  -0.47367361 -0.48885426 -1.35557354\n",
      "  -0.30756234  0.61583843]\n",
      " [-1.56610693  0.4333115   0.43279337 -0.47367361  0.42073024 -1.35557354\n",
      "  -0.30756234  0.61583843]\n",
      " [ 0.82737724  0.4333115  -0.4745452  -0.47367361 -0.48633742  0.73769513\n",
      "  -0.30756234  0.61583843]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Đọc dữ liệu từ file CSV\n",
    "train_data = pd.read_csv('/kaggle/input/2024-hk-1-se-355-p-11-titanic-classification/train.csv')\n",
    "\n",
    "# Xử lý giá trị thiếu\n",
    "train_data['Age'] = train_data['Age'].fillna(train_data['Age'].median())\n",
    "train_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].mode()[0])\n",
    "train_data['Fare'] = train_data['Fare'].fillna(train_data['Fare'].median())\n",
    "\n",
    "# Chuyển đổi các biến phân loại thành số\n",
    "train_data = pd.get_dummies(train_data, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# Loại bỏ các cột không cần thiết\n",
    "train_data.drop(columns=['Name', 'Ticket', 'Cabin'], inplace=True)\n",
    "\n",
    "# Tách dữ liệu thành các đặc trưng và biến mục tiêu\n",
    "X = train_data.drop(columns=['Survived', 'PassengerId'])\n",
    "y = train_data['Survived']\n",
    "\n",
    "# Chuẩn hóa các đặc trưng\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Hiển thị một vài dòng đầu tiên của dữ liệu đã xử lý để kiểm tra\n",
    "print(train_data.head())\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72ccb3d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T04:03:46.444986Z",
     "iopub.status.busy": "2025-01-19T04:03:46.444550Z",
     "iopub.status.idle": "2025-01-19T04:03:46.793639Z",
     "shell.execute_reply": "2025-01-19T04:03:46.792411Z"
    },
    "papermill": {
     "duration": 0.361386,
     "end_time": "2025-01-19T04:03:46.795820",
     "exception": false,
     "start_time": "2025-01-19T04:03:46.434434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.7877094972067039\n",
      "Decision Tree Classifier Accuracy: 0.7988826815642458\n",
      "Random Forest Classifier Accuracy: 0.7988826815642458\n",
      "Support Vector Classifier Accuracy: 0.770949720670391\n",
      "K-Nearest Neighbors Classifier Accuracy: 0.8044692737430168\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Hàm để huấn luyện và đánh giá mô hình\n",
    "def train_and_evaluate_model(model, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "    return model\n",
    "\n",
    "# Huấn luyện và đánh giá các mô hình khác nhau\n",
    "\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg_model = train_and_evaluate_model(logreg, \"Logistic Regression\")\n",
    "\n",
    "# Decision Tree Classifier\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree_model = train_and_evaluate_model(dtree, \"Decision Tree Classifier\")\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier()\n",
    "rf_model = train_and_evaluate_model(rf, \"Random Forest Classifier\")\n",
    "\n",
    "# Support Vector Classifier\n",
    "svc = SVC()\n",
    "svc_model = train_and_evaluate_model(svc, \"Support Vector Classifier\")\n",
    "\n",
    "# K-Nearest Neighbors Classifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn_model = train_and_evaluate_model(knn, \"K-Nearest Neighbors Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67ac896d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T04:03:46.815523Z",
     "iopub.status.busy": "2025-01-19T04:03:46.815029Z",
     "iopub.status.idle": "2025-01-19T04:03:47.175894Z",
     "shell.execute_reply": "2025-01-19T04:03:47.174511Z"
    },
    "papermill": {
     "duration": 0.372999,
     "end_time": "2025-01-19T04:03:47.177708",
     "exception": false,
     "start_time": "2025-01-19T04:03:46.804709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8100558659217877\n",
      "Decision Tree Classifier Accuracy: 0.7877094972067039\n",
      "Random Forest Classifier Accuracy: 0.8212290502793296\n",
      "Support Vector Classifier Accuracy: 0.8100558659217877\n",
      "K-Nearest Neighbors Classifier Accuracy: 0.8044692737430168\n",
      "Submission file for Logistic_Regression created successfully.\n",
      "Submission file for Decision_Tree_Classifier created successfully.\n",
      "Submission file for Random_Forest_Classifier created successfully.\n",
      "Submission file for Support_Vector_Classifier created successfully.\n",
      "Submission file for K_Nearest_Neighbors_Classifier created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Đọc dữ liệu từ file CSV\n",
    "train_data = pd.read_csv('/kaggle/input/2024-hk-1-se-355-p-11-titanic-classification/train.csv')\n",
    "\n",
    "# Xử lý giá trị thiếu\n",
    "train_data['Age'] = train_data['Age'].fillna(train_data['Age'].median())\n",
    "train_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].mode()[0])\n",
    "train_data['Fare'] = train_data['Fare'].fillna(train_data['Fare'].median())\n",
    "\n",
    "# Chuyển đổi các biến phân loại thành số\n",
    "train_data = pd.get_dummies(train_data, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# Loại bỏ các cột không cần thiết\n",
    "train_data.drop(columns=['Name', 'Ticket', 'Cabin'], inplace=True)\n",
    "\n",
    "# Tách dữ liệu thành các đặc trưng và biến mục tiêu\n",
    "X = train_data.drop(columns=['Survived', 'PassengerId'])\n",
    "y = train_data['Survived']\n",
    "\n",
    "# Chuẩn hóa các đặc trưng\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Tách dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hàm để huấn luyện và đánh giá mô hình\n",
    "def train_and_evaluate_model(model, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "    return model\n",
    "\n",
    "# Huấn luyện và đánh giá các mô hình khác nhau\n",
    "\n",
    "# Logistic Regression (tăng số lượng vòng lặp tối đa)\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg_model = train_and_evaluate_model(logreg, \"Logistic Regression\")\n",
    "\n",
    "# Decision Tree Classifier\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree_model = train_and_evaluate_model(dtree, \"Decision Tree Classifier\")\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier()\n",
    "rf_model = train_and_evaluate_model(rf, \"Random Forest Classifier\")\n",
    "\n",
    "# Support Vector Classifier\n",
    "svc = SVC()\n",
    "svc_model = train_and_evaluate_model(svc, \"Support Vector Classifier\")\n",
    "\n",
    "# K-Nearest Neighbors Classifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn_model = train_and_evaluate_model(knn, \"K-Nearest Neighbors Classifier\")\n",
    "\n",
    "# Đọc dữ liệu test từ file CSV\n",
    "test_data = pd.read_csv('/kaggle/input/2024-hk-1-se-355-p-11-titanic-classification/test.csv')\n",
    "\n",
    "# Xử lý giá trị thiếu trong dữ liệu test\n",
    "test_data['Age'] = test_data['Age'].fillna(test_data['Age'].median())\n",
    "test_data['Embarked'] = test_data['Embarked'].fillna(test_data['Embarked'].mode()[0])\n",
    "test_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].median())\n",
    "\n",
    "# Chuyển đổi các biến phân loại thành số trong dữ liệu test\n",
    "test_data = pd.get_dummies(test_data, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# Đảm bảo rằng dữ liệu test có cùng các cột với dữ liệu huấn luyện\n",
    "missing_cols = set(train_data.columns) - set(test_data.columns)\n",
    "for col in missing_cols:\n",
    "    test_data[col] = 0\n",
    "test_data = test_data[train_data.columns.drop(['Survived', 'PassengerId'])]\n",
    "\n",
    "# Chuẩn hóa dữ liệu test\n",
    "X_test = scaler.transform(test_data)\n",
    "\n",
    "# Hàm để tạo file submission từ mô hình đã huấn luyện\n",
    "def create_submission_file(model, model_name):\n",
    "    predictions = model.predict(X_test)\n",
    "    submission = pd.DataFrame({'PassengerId': test_data.index + 892, 'Survived': predictions})\n",
    "    submission.to_csv(f'submission_{model_name}.csv', index=False)\n",
    "    print(f\"Submission file for {model_name} created successfully.\")\n",
    "\n",
    "# Tạo file submission cho từng mô hình\n",
    "\n",
    "create_submission_file(logreg_model, \"Logistic_Regression\")\n",
    "create_submission_file(dtree_model, \"Decision_Tree_Classifier\")\n",
    "create_submission_file(rf_model, \"Random_Forest_Classifier\")\n",
    "create_submission_file(svc_model, \"Support_Vector_Classifier\")\n",
    "create_submission_file(knn_model, \"K_Nearest_Neighbors_Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "657a39a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T04:03:47.198119Z",
     "iopub.status.busy": "2025-01-19T04:03:47.197671Z",
     "iopub.status.idle": "2025-01-19T04:03:47.254316Z",
     "shell.execute_reply": "2025-01-19T04:03:47.252795Z"
    },
    "papermill": {
     "duration": 0.069531,
     "end_time": "2025-01-19T04:03:47.256557",
     "exception": false,
     "start_time": "2025-01-19T04:03:47.187026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu huấn luyện đã tiền xử lý:\n",
      "   PassengerId  Survived  Pclass   Age  SibSp  Parch     Fare  FamilySize  \\\n",
      "0            1         0       3  22.0      1      0   7.2500           2   \n",
      "1            2         1       1  38.0      1      0  71.2833           2   \n",
      "2            3         1       3  26.0      0      0   7.9250           1   \n",
      "3            4         1       1  35.0      1      0  53.1000           2   \n",
      "4            5         0       3  35.0      0      0   8.0500           1   \n",
      "\n",
      "   IsAlone  Sex_male  Embarked_Q  Embarked_S  \n",
      "0        0      True       False        True  \n",
      "1        0     False       False       False  \n",
      "2        1     False       False        True  \n",
      "3        0     False       False        True  \n",
      "4        1      True       False        True  \n",
      "\n",
      "Dữ liệu test đã tiền xử lý:\n",
      "   PassengerId  Pclass   Age  SibSp  Parch     Fare  FamilySize  IsAlone  \\\n",
      "0          892       3  34.5      0      0   7.8292           1        1   \n",
      "1          893       3  47.0      1      0   7.0000           2        0   \n",
      "2          894       2  62.0      0      0   9.6875           1        1   \n",
      "3          895       3  27.0      0      0   8.6625           1        1   \n",
      "4          896       3  22.0      1      1  12.2875           3        0   \n",
      "\n",
      "   Sex_male  Embarked_Q  Embarked_S  \n",
      "0      True        True       False  \n",
      "1     False       False        True  \n",
      "2      True        True       False  \n",
      "3      True       False        True  \n",
      "4     False       False        True  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Đọc dữ liệu từ file CSV\n",
    "train_data = pd.read_csv('/kaggle/input/2024-hk-1-se-355-p-11-titanic-classification/train.csv')\n",
    "test_data = pd.read_csv('/kaggle/input/2024-hk-1-se-355-p-11-titanic-classification/test.csv')\n",
    "\n",
    "# Xử lý giá trị thiếu trong dữ liệu huấn luyện\n",
    "train_data['Age'] = train_data['Age'].fillna(train_data['Age'].median())\n",
    "train_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].mode()[0])\n",
    "train_data['Fare'] = train_data['Fare'].fillna(train_data['Fare'].median())\n",
    "\n",
    "# Tạo thêm đặc trưng mới trong dữ liệu huấn luyện\n",
    "train_data['FamilySize'] = train_data['SibSp'] + train_data['Parch'] + 1\n",
    "train_data['IsAlone'] = (train_data['FamilySize'] == 1).astype(int)\n",
    "\n",
    "# Chuyển đổi các biến phân loại thành số trong dữ liệu huấn luyện\n",
    "train_data = pd.get_dummies(train_data, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# Loại bỏ các cột không cần thiết trong dữ liệu huấn luyện\n",
    "train_data.drop(columns=['Name', 'Ticket', 'Cabin'], inplace=True)\n",
    "\n",
    "# Tách dữ liệu thành các đặc trưng và biến mục tiêu trong dữ liệu huấn luyện\n",
    "X = train_data.drop(columns=['Survived', 'PassengerId'])\n",
    "y = train_data['Survived']\n",
    "\n",
    "# Chuẩn hóa các đặc trưng trong dữ liệu huấn luyện\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Tách dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Xử lý giá trị thiếu trong dữ liệu test\n",
    "test_data['Age'] = test_data['Age'].fillna(test_data['Age'].median())\n",
    "test_data['Embarked'] = test_data['Embarked'].fillna(test_data['Embarked'].mode()[0])\n",
    "test_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].median())\n",
    "\n",
    "# Tạo thêm đặc trưng mới trong dữ liệu test\n",
    "test_data['FamilySize'] = test_data['SibSp'] + test_data['Parch'] + 1\n",
    "test_data['IsAlone'] = (test_data['FamilySize'] == 1).astype(int)\n",
    "\n",
    "# Chuyển đổi các biến phân loại thành số trong dữ liệu test\n",
    "test_data = pd.get_dummies(test_data, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# Đảm bảo rằng dữ liệu test có cùng các cột với dữ liệu huấn luyện\n",
    "missing_cols = set(train_data.columns) - set(test_data.columns)\n",
    "for col in missing_cols:\n",
    "    test_data[col] = 0\n",
    "test_data = test_data[train_data.columns.drop(['Survived'])]\n",
    "\n",
    "# Chuẩn hóa dữ liệu test\n",
    "X_test = scaler.transform(test_data.drop(columns=['PassengerId']))\n",
    "\n",
    "# Hiển thị dữ liệu huấn luyện đã tiền xử lý\n",
    "print(\"Dữ liệu huấn luyện đã tiền xử lý:\")\n",
    "print(train_data.head())\n",
    "\n",
    "# Hiển thị dữ liệu test đã tiền xử lý\n",
    "print(\"\\nDữ liệu test đã tiền xử lý:\")\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "225d55b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T04:03:47.277126Z",
     "iopub.status.busy": "2025-01-19T04:03:47.276622Z",
     "iopub.status.idle": "2025-01-19T04:03:47.546533Z",
     "shell.execute_reply": "2025-01-19T04:03:47.545028Z"
    },
    "papermill": {
     "duration": 0.282659,
     "end_time": "2025-01-19T04:03:47.548710",
     "exception": false,
     "start_time": "2025-01-19T04:03:47.266051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.7988826815642458\n",
      "Decision Tree Classifier Accuracy: 0.776536312849162\n",
      "Random Forest Classifier Accuracy: 0.8212290502793296\n",
      "Support Vector Classifier Accuracy: 0.8156424581005587\n",
      "K-Nearest Neighbors Classifier Accuracy: 0.8100558659217877\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Hàm để huấn luyện và đánh giá mô hình\n",
    "def train_and_evaluate_model(model, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
    "    return model\n",
    "\n",
    "# Huấn luyện và đánh giá các mô hình khác nhau\n",
    "\n",
    "# Logistic Regression (tăng số lượng vòng lặp tối đa)\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg_model = train_and_evaluate_model(logreg, \"Logistic Regression\")\n",
    "\n",
    "# Decision Tree Classifier\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree_model = train_and_evaluate_model(dtree, \"Decision Tree Classifier\")\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier()\n",
    "rf_model = train_and_evaluate_model(rf, \"Random Forest Classifier\")\n",
    "\n",
    "# Support Vector Classifier\n",
    "svc = SVC()\n",
    "svc_model = train_and_evaluate_model(svc, \"Support Vector Classifier\")\n",
    "\n",
    "# K-Nearest Neighbors Classifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn_model = train_and_evaluate_model(knn, \"K-Nearest Neighbors Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a808256",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-19T04:03:47.570542Z",
     "iopub.status.busy": "2025-01-19T04:03:47.570017Z",
     "iopub.status.idle": "2025-01-19T04:03:47.637883Z",
     "shell.execute_reply": "2025-01-19T04:03:47.636277Z"
    },
    "papermill": {
     "duration": 0.080861,
     "end_time": "2025-01-19T04:03:47.640040",
     "exception": false,
     "start_time": "2025-01-19T04:03:47.559179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file for Logistic_Regression created successfully.\n",
      "Submission file for Decision_Tree_Classifier created successfully.\n",
      "Submission file for Random_Forest_Classifier created successfully.\n",
      "Submission file for Support_Vector_Classifier created successfully.\n",
      "Submission file for K_Nearest_Neighbors_Classifier created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Hàm để tạo file submission từ mô hình đã huấn luyện\n",
    "def create_submission_file(model, model_name):\n",
    "    predictions = model.predict(X_test)\n",
    "    submission = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': predictions})\n",
    "    submission.to_csv(f'submission_{model_name}.csv', index=False)\n",
    "    print(f\"Submission file for {model_name} created successfully.\")\n",
    "\n",
    "# Tạo file submission cho từng mô hình\n",
    "\n",
    "create_submission_file(logreg_model, \"Logistic_Regression\")\n",
    "create_submission_file(dtree_model, \"Decision_Tree_Classifier\")\n",
    "create_submission_file(rf_model, \"Random_Forest_Classifier\")\n",
    "create_submission_file(svc_model, \"Support_Vector_Classifier\")\n",
    "create_submission_file(knn_model, \"K_Nearest_Neighbors_Classifier\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10576436,
     "sourceId": 90741,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 44.284016,
   "end_time": "2025-01-19T04:03:50.061675",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-19T04:03:05.777659",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
